{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1uTPXFla17IYiM7CUvLOpZvnHiZyKWStL","timestamp":1683923990288},{"file_id":"1r1cXR7dzThRQgHG09TTnQSiq2W1L1q0s","timestamp":1683911596724}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"OgexxksFNxVL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683906069367,"user_tz":-180,"elapsed":5532,"user":{"displayName":"Eng rana Khaled","userId":"13019899431006653452"}},"outputId":"cb9a8aae-cda4-43a2-cd5b-673417fc5479"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tweet-preprocessor in /usr/local/lib/python3.10/dist-packages (0.6.0)\n"]}],"source":["  #import libirary \"panda\" to use data set\n","import pandas as pd\n","  #we need this lib to clean the dataset\n","!pip install tweet-preprocessor\n","import preprocessor as p\n","import numpy as np\n","import sklearn\n","from sklearn.svm import SVC\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","source":["import nltk\n","!pip install googletrans==4.0.0-rc1\n","nltk.download('punkt')  \n","nltk.download('averaged_perceptron_tagger') \n","nltk.download('wordnet') \n","nltk.download('stopwords')\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1683912080136,"user_tz":-180,"elapsed":11469,"user":{"displayName":"Mohammed Mostafa","userId":"17827135915400521959"}},"outputId":"1125dee7-1e7f-4bee-a22f-4b9233a4c06f","id":"y_4NeLA0EjAR"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting googletrans==4.0.0-rc1\n","  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n","  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2022.12.7)\n","Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading hstspreload-2023.1.1-py3-none-any.whl (1.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.0)\n","Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n","Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n","Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n","  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n","Building wheels for collected packages: googletrans\n","  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17397 sha256=3a722270d4a71a006826becf586a4fe10d245ede7fac86d0c091148db9b12056\n","  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\n","Successfully built googletrans\n","Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n","  Attempting uninstall: chardet\n","    Found existing installation: chardet 4.0.0\n","    Uninstalling chardet-4.0.0:\n","      Successfully uninstalled chardet-4.0.0\n","  Attempting uninstall: idna\n","    Found existing installation: idna 3.4\n","    Uninstalling idna-3.4:\n","      Successfully uninstalled idna-3.4\n","Successfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2023.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["chardet","idna"]}}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import wordnet\n","from nltk.corpus import stopwords\n","\n","stop_words=stopwords.words(\"english\") \n","lemmatizer = WordNetLemmatizer()\n"],"metadata":{"id":"3_UHQQIVFOgU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["  #we called the data set in our project\n","df=pd.read_csv('dataset2.csv',encoding='utf-8')\n"],"metadata":{"id":"bCGMnwvVOJ0i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"FOdWhb4_e19v","executionInfo":{"status":"ok","timestamp":1683906074963,"user_tz":-180,"elapsed":35,"user":{"displayName":"Eng rana Khaled","userId":"13019899431006653452"}},"outputId":"3f9a6917-ac92-4347-d4d8-33dabe78a20b","colab":{"base_uri":"https://localhost:8080/","height":206}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["  English    Arbic\n","0     Hi.  مرحبًا.\n","1    Run!    اركض!\n","2   Help!  النجدة!\n","3   Jump!    اقفز!\n","4   Stop!      قف!"],"text/html":["\n","  <div id=\"df-c115419b-4215-469e-86f1-9b90112e346e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>English</th>\n","      <th>Arbic</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Hi.</td>\n","      <td>مرحبًا.</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Run!</td>\n","      <td>اركض!</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Help!</td>\n","      <td>النجدة!</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Jump!</td>\n","      <td>اقفز!</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Stop!</td>\n","      <td>قف!</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c115419b-4215-469e-86f1-9b90112e346e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c115419b-4215-469e-86f1-9b90112e346e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c115419b-4215-469e-86f1-9b90112e346e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["#we set the input and the output variables\n","x =df[\"English\"]\n","y =df[\"Arbic\"]"],"metadata":{"id":"ndeQSWBJ-jwt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["  #we call function apply from preprocessor to clean the data\n","  #we will clean the input lambda (Anonymous function)\n","x_preprocessed=x.apply(lambda word:p.clean(word))"],"metadata":{"id":"53sx-HNjgIiz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#we will apply the TF_IDF to prepare the data \n","#1-import the sklearn lib\n","from sklearn.feature_extraction.text import TfidfVectorizer\n"],"metadata":{"id":"l33kzBC5ayst"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["  #3- we will put the result of tfidfin variable to can we use\n","tfidf=TfidfVectorizer(encoding='utf-8',ngram_range=(1,2),stop_words='english')\n","  #we will fit the input of data set\n","tf_idf=tfidf.fit(x_preprocessed)\n","  #we will apply the tf_idf in all of inputROW that we cleaned\n","english_input_fit=tf_idf.transform(x_preprocessed)\n","\n"],"metadata":{"id":"8At0uNdHnnIL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def nltk_tag_to_wordnet_tag(nltk_tag):  \n"," if nltk_tag.startswith('J'): \n","    return wordnet.ADJ\n"," elif nltk_tag.startswith('V'): \n","    return wordnet.VERB\n"," elif nltk_tag.startswith('N'): \n","    return wordnet.NOUN\n"," elif nltk_tag.startswith('R'):\n","    return wordnet.ADV  \n"," else:\n","    return None\n"," "],"metadata":{"id":"SYVTjAiv_p3e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def lemmatize_sentence(sentence):\n"," nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n"," wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)  \n"," lemmatized_sentence =  []\n"," for word, tag in wordnet_tagged:\n","  if word not in stop_words:\n","   if tag is None:\n","     lemmatized_sentence.append(word)\n","   else:\n","     lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))  \n","  return \" \".join(lemmatized_sentence)\n"],"metadata":{"id":"_UQCTjLIRARr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_cleaned = x.apply(lemmatize_sentence)"],"metadata":{"id":"LaDu6LwoQlLu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Machine Learning"],"metadata":{"id":"L5wzLDS6WFpm"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","tfidf = TfidfVectorizer(encoding='latin-1',ngram_range=(1,2), stop_words='english')\n","tfidf = tfidf.fit (x_cleaned)\n","\n","x_tfidf=tfidf.transform(x_cleaned)\n","\n","x_tfidf.shape\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"KtVZlEGYVayg","executionInfo":{"status":"error","timestamp":1683918208309,"user_tz":-180,"elapsed":509,"user":{"displayName":"Mohammed Mostafa","userId":"17827135915400521959"}},"outputId":"5a5c96c5-bff4-4807-a4e2-3651896148bd"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-da0720ab203a>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin-1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_cleaned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mx_tfidf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cleaned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'x_cleaned' is not defined"]}]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","x_train,x_test,y_train,y_test=train_test_split(x_tfidf,y,test_size=0.3)\n"],"metadata":{"id":"YndBgoMXbieg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = pd.read_csv('dataset2.csv')\n","from sklearn.svm import SVC\n","# Create an SVM classifier and fit it to the training data\n","clf = SVC(kernel='linear')\n","clf.fit(X_train, y_train)\n","y_pred = clf.predict(X_test)\n","# Evaluate the performance of the model\n","accuracy = clf.score(X_test, y_test)\n","print(\"Accuracy:\", accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QvjbR5zlrZRt","executionInfo":{"status":"ok","timestamp":1683907738999,"user_tz":-180,"elapsed":384,"user":{"displayName":"Eng rana Khaled","userId":"13019899431006653452"}},"outputId":"2a4e8d01-71c1-4276-e436-3b27e09f95ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 1.0\n"]}]},{"cell_type":"markdown","source":["sequence to sequence"],"metadata":{"id":"KqQJALygWWg0"}},{"cell_type":"code","source":["\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from googletrans import Translator\n","from sklearn.metrics import accuracy_score\n","\n","# تحديد المعلمات الأساسية\n","vocab_size = 10000  # عدد الكلمات في القاموس\n","max_len = 100  # الطول الأقصى للجمل\n","embedding_dim = 256  # الأبعاد لتدريب النموذج\n","units = 1024  # عدد الوحدات في الطبقة الأخيرة\n","\n","# إنشاء النموذج Seq2Seq\n","\n","def build_model():\n","    # طبقة التشفير (encoder)\n","    encoder_inputs = keras.Input(shape=(None,))\n","    encoder_embeddings = layers.Embedding(vocab_size, embedding_dim)(encoder_inputs)\n","    encoder_lstm = layers.LSTM(units, return_state=True)\n","    encoder_outputs, state_h, state_c = encoder_lstm(encoder_embeddings)\n","    encoder_states = [state_h, state_c]\n","\n","    # طبقة الفك (decoder)\n","    decoder_inputs = keras.Input(shape=(None,))\n","    decoder_embeddings = layers.Embedding(vocab_size, embedding_dim)(decoder_inputs)\n","    decoder_lstm = layers.LSTM(units, return_sequences=True, return_state=True)\n","    decoder_outputs, _, _ = decoder_lstm(decoder_embeddings, initial_state=encoder_states)\n","    decoder_dense = layers.Dense(vocab_size, activation=\"softmax\")\n","    decoder_outputs = decoder_dense(decoder_outputs)\n","\n","    # جمع النموذج\n","    model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","    return model\n","\n","# تدريب النموذج Seq2Seq\n","def train_model(model, x_train, y_train):\n","    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n","    model.fit(x_train, y_train, batch_size=64, epochs=10, validation_split=0.2)\n","\n","# استخدام النموذج Seq2Seq للترجمة\n","def translate(model, sentence, input_tokenizer, target_tokenizer):\n","    # تحويل الجملة الأصلية إلى تتابع (sequence) من الأرقام\n","    sequence = input_tokenizer.texts_to_sequences([sentence])\n","    sequence = keras.preprocessing.sequence.pad_sequences(sequence, maxlen=max_len, padding=\"post\")\n","\n","    # تشغيل الجملة عبر النموذج\n","    decoded_sequence = []\n","    target_sequence = target_tokenizer.word_index[\"<start>\"]\n","    for i in range(max_len):\n","        output_sequence = model.predict([sequence, [target_sequence]])\n","        target_sequence = tf.argmax(output_sequence[0, i]).numpy()\n","        decoded_sequence.append(target_sequence)\n","        if target_sequence == target_tokenizer.word_index[\"<end>\"]:\n","            break\n","\n","    # تحويل التتابع الناتج إلى جملة\n","    decoded_sequence = target_tokenizer.sequences_to_texts([decoded_sequence])\n","    return decoded_sequence[0]\n","    \n","\n","\n"],"metadata":{"id":"R40x1iBivxxj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"sq3CH2FDKNSE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","translator = Translator()\n","\n","# طلب إدخال النص من المستخدم\n","text = input(\"Enter Text: \")\n","\n","# طلب إدخال اللغة المستهدفة من المستخدم\n","target_lang = input(\"Enter Target Language (e.g. fr, ar): \")\n","\n","# ترجمة النص باستخدام المكتبة\n","translated = translator.translate(text, dest=target_lang)\n","\n","# عرض النص المترجم\n","print(translated.text)"],"metadata":{"id":"E94mra_Z45pr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Recursive Neural Networks (Tree-based Models)"],"metadata":{"id":"giPF_Ksv7W-u"}},{"cell_type":"code","source":[],"metadata":{"id":"HK_T05CA7kGr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras import layers\n","from googletrans import Translator\n","# تحديد المعلمات الأساسية\n","vocab_size = 10000  # عدد الكلمات في القاموس\n","embedding_dim = 256  # الأبعاد لتدريب النموذج\n","units = 1024  # عدد الوحدات في الطبقة الأخيرة\n","\n","# إنشاء النموذج Recursive Neural Network\n","def build_model():\n","    # طبقة التضمين (Embedding)\n","    inputs = layers.Input(shape=(None,))\n","    embeddings = layers.Embedding(vocab_size, embedding_dim)(inputs)\n","\n","    # طبقة الشجرة (Tree)\n","    tree = layers.Lambda(lambda x: tf.squeeze(tf.keras.backend.batch_dot(x[:, :-1, :], tf.expand_dims(x[:, 1:, :], 2), axes=(2, 3))), name='Tree')(embeddings)\n","\n","    # طبقة الطبقة الأخيرة (Dense)\n","    outputs = layers.Dense(vocab_size, activation=\"softmax\")(tree)\n","\n","    # جمع النموذج\n","    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n","    return model\n","\n","# تدريب النموذج Recursive Neural Network\n","def train_model(model, x_train, y_train):\n","    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n","    model.fit(x_train, y_train, batch_size=64, epochs=10, validation_split=0.2)\n","\n","# استخدام النموذج Recursive Neural Network للترجمة\n","def translate(model, sentence, input_tokenizer, target_tokenizer):\n","    # تحويل الجملة الأصلية إلى تتابع (sequence) من الأرقام\n","    sequence = input_tokenizer.texts_to_sequences([sentence])\n","    sequence = tf.keras.preprocessing.sequence.pad_sequences(sequence, padding='post')\n","\n","    # تشغيل الجملة عبر النموذج\n","    y = model.predict(sequence)\n","    y = tf.squeeze(output, 0).numpy()\n","\n","    # تحويل التتابع الناتج إلى جملة\n","    decoded_sequence = target_tokenizer.sequences_to_texts([tf.argmax(output, axis=-1).numpy()])\n","    return decoded_sequence[0]\n","    # تقسيم البيانات إلى مجموعات التدريب والاختبار\n","x_train, x_test, y_train, y_test = train_test_split(input_sequences, target_sequences, test_size=0.2, random_state=42)\n","\n","# إنشاء وتدريب النموذج\n","model = build_model()\n","train_model(model, x_train, y_train, x_test, y_test)\n","\n","# استخدام النموذج للترجمة\n","sentence = \"This restaurant is great!\"\n","translated_sentence = translate(model, sentence, input_tokenizer, target_tokenizer)\n","print(translated_sentence)"],"metadata":{"id":"OwQDfZwbV5Yb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0LipagSN71rn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","translator = Translator()\n","\n","# طلب إدخال النص من المستخدم\n","text = input(\"Enter Text: \")\n","\n","# طلب إدخال اللغة المستهدفة من المستخدم\n","target_lang = input(\"Enter Target Language (e.g., fr, ar, es): \")\n","\n","# ترجمة النص باستخدام المكتبة\n","translated = translator.translate(text, dest=target_lang)\n","\n","# عرض النص المترجم\n","print(translated.text)"],"metadata":{"id":"d11dC1U772LP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Transformer Networks"],"metadata":{"id":"U2FGC8Lw73iW"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers, models\n","from googletrans import Translator\n","# تحديد المعلمات الأساسية\n","vocab_size = 10000  # عدد الكلمات في القاموس\n","max_seq_len = 100  # الطول الأقصى للجملة\n","num_heads = 8  # عدد الرؤوس في طبقة Multi-Head Attention\n","ff_dim = 512  # عدد الوحدات في طبقة Feed-Forward\n","\n","# إنشاء النموذج Transformer\n","def build_transformer_model():\n","    # طبقة التضمين (Embedding)\n","    inputs = layers.Input(shape=(max_seq_len,), dtype=\"int32\")\n","    x = layers.Embedding(vocab_size, ff_dim)(inputs)\n","    x = layers.Dropout(0.1)(x)\n","\n","    # طبقة الترميز (Encoding)\n","    x = layers.LayerNormalization(epsilon=1e-6)(x)\n","    x = layers.MultiHeadAttention(num_heads=num_heads, key_dim=ff_dim)(x, x)\n","    x = layers.Dropout(0.1)(x)\n","    x1 = layers.LayerNormalization(epsilon=1e-6)(x)\n","    x = layers.Dense(ff_dim, activation=\"relu\")(x)\n","    x = layers.Dense(ff_dim)(x)\n","    x = layers.Dropout(0.1)(x)\n","    x = layers.Add()([x1, x])\n","\n","    # طبقة التفكيك (Decoding)\n","    x = layers.LayerNormalization(epsilon=1e-6)(x)\n","    x = layers.MultiHeadAttention(num_heads=num_heads, key_dim=ff_dim)(x, x)\n","    x = layers.Dropout(0.1)(x)\n","    x1 = layers.LayerNormalization(epsilon=1e-6)(x)\n","    x = layers.Dense(ff_dim, activation=\"relu\")(x)\n","    x = layers.Dense(ff_dim)(x)\n","    x = layers.Dropout(0.1)(x)\n","    x = layers.Add()([x1, x])\n","\n","    # طبقة الإخراج (Output)\n","    outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n","\n","    # جمع النموذج\n","    model = models.Model(inputs=inputs, outputs=outputs)\n","    return model\n","\n","# تدريب النموذج Transformer\n","def train_transformer_model(model, x_train, y_train):\n","    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n","    model.fit(x_train, y_train, batch_size=64, epochs=10, validation_split=0.2)\n","\n","# استخدام النموذج Transformer للترجمة\n","def translate_transformer(model, sentence, input_tokenizer, target_tokenizer):\n","    # تحويل الجملة الأصلية إلى تتابع (sequence) من الأرقام\n","    sequence = input_tokenizer.texts_to_sequences([sentence])\n","    sequence = keras.preprocessing.sequence.pad_sequences(sequence, maxlen=max_seq_len, padding=\"post\")\n","\n","    # تشغيل الجملة عبر النموذج\n","    decoded_sequence = []\n","    target_sequence = target_tokenizer.word_index[\"<start>\"]\n","    for i in range(max_seq_len):\n","        output_sequence = model.predict(sequence)\n","        target_sequence = tf.argmax(output_sequence[0, i]).numpy()\n","        decoded_sequence.append(target_sequence)\n","        if target_sequence == target_tokenizer.word_index[\"<end>\"]:\n","            break\n","\n","    # تحويل التتابع الناتج إلى جملة\n","    decoded_sequence = target_tokenizer.sequences_to_texts([decoded_sequence])\n","    return decoded_sequence[0]"],"metadata":{"id":"bZ71EKVm79Sa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"YztaGezb8GV4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","translator = Translator()\n","\n","# طلب إدخال النص من المستخدم\n","text = input(\"Enter Text: \")\n","\n","# طلب إدخال اللغة المستهدفة من المستخدم\n","target_lang = input(\"Enter Target Language (e.g., fr, ar, es): \")\n","\n","# ترجمة النص باستخدام المكتبة\n","translated = translator.translate(text, dest=target_lang)\n","\n","# عرض النص المترجم\n","print(translated.text)"],"metadata":{"id":"icsC0Oh38NuY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Recurrent Neural Networks (RNNs):"],"metadata":{"id":"LrkF-FuF8Ovc"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers, models\n","from googletrans import Translator\n","# تحديد المعلمات الأساسية\n","vocab_size = 10000  # عدد الكلمات في القاموس\n","embedding_dim = 256  # الأبعاد لتدريب النموذج\n","units = 1024  # عدد الوحدات في الطبقة الأخيرة\n","\n","# إنشاء النموذج Recurrent Neural Network\n","def build_rnn_model():\n","    # طبقة التضمين (Embedding)\n","    inputs = layers.Input(shape=(None,))\n","    embeddings = layers.Embedding(vocab_size, embedding_dim)(inputs)\n","\n","    # طبقة الـLSTM\n","    lstm = layers.LSTM(units)(embeddings)\n","\n","    # طبقة الطبقة الأخيرة (Dense)\n","    outputs = layers.Dense(vocab_size, activation=\"softmax\")(lstm)\n","\n","    # جمع النموذج\n","    model = models.Model(inputs=inputs, outputs=outputs)\n","    return model\n","\n","# تدريب النموذج Recurrent Neural Network\n","def train_rnn_model(model, x_train, y_train):\n","    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n","    model.fit(x_train, y_train, batch_size=64, epochs=10, validation_split=0.2)\n","\n","# استخدام النموذج Recurrent Neural Network للترجمة\n","def translate_rnn(model, sentence, input_tokenizer, target_tokenizer):\n","    # تحويل الجملة الأصلية إلى تتابع (sequence) من الأرقام\n","    sequence = input_tokenizer.texts_to_sequences([sentence])\n","    sequence = keras.preprocessing.sequence.pad_sequences(sequence, padding='post')\n","\n","    # تشغيل الجملة عبر النموذج\n","    output = model.predict(sequence)\n","    output = tf.squeeze(output, 0).numpy()\n","\n","    # تحويل التتابع الناتج إلى جملة\n","    decoded_sequence = target_tokenizer.sequences_to_texts([tf.argmax(output, axis=-1).numpy()])\n","    return decoded_sequence[0]"],"metadata":{"id":"Ih7FMtRl8Vcx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Y7KGzQqL8cOB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","translator = Translator()\n","\n","# طلب إدخال النص من المستخدم\n","text = input(\"Enter Text: \")\n","\n","# طلب إدخال اللغة المستهدفة من المستخدم\n","target_lang = input(\"Enter Target Language (e.g., fr, ar, es): \")\n","\n","# ترجمة النص باستخدام المكتبة\n","translated = translator.translate(text, dest=target_lang)\n","\n","# عرض النص المترجم\n","print(translated.text)"],"metadata":{"id":"APRkhCed8igY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Convolutional Neural Networks (CNNs):"],"metadata":{"id":"OY2n0GzM8jHV"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras import layers, models\n","from googletrans import Translator\n","# تحديد المعلمات الأساسية\n","img_height = 224\n","img_width = 224\n","num_classes = 10\n","\n","# إنشاء النموذج Convolutional Neural Network\n","def build_cnn_model():\n","    # طبقة الإدخال (Input)\n","    inputs = layers.Input(shape=(img_height, img_width, 3))\n","\n","    # طبقة الـConvolution\n","    x = layers.Conv2D(32, (3, 3), activation='relu')(inputs)\n","    x = layers.MaxPooling2D((2, 2))(x)\n","    x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n","    x = layers.MaxPooling2D((2, 2))(x)\n","    x = layers.Conv2D(128, (3, 3), activation='relu')(x)\n","    x = layers.MaxPooling2D((2, 2))(x)\n","    x = layers.Conv2D(128, (3, 3), activation='relu')(x)\n","    x = layers.MaxPooling2D((2, 2))(x)\n","\n","    # طبقة التفريغ (Flatten)\n","    x = layers.Flatten()(x)\n","\n","    # طبقة الإخراج (Output)\n","    outputs = layers.Dense(num_classes, activation='softmax')(x)\n","\n","    # جمع النموذج\n","    model = models.Model(inputs=inputs, outputs=outputs)\n","    return model\n","\n","# تدريب النموذج Convolutional Neural Network\n","def train_cnn_model(model, train_data, validation_data):\n","    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","    model.fit(train_data, validation_data=validation_data, epochs=10)\n","\n","# تقييم النموذج Convolutional Neural Network\n","def evaluate_cnn_model(model, test_data):\n","    loss, accuracy = model.evaluate(test_data)\n","    print('Test accuracy:', accuracy)"],"metadata":{"id":"zBPt5gPx8qf2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_itDvEYG811h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["translator = Translator()\n","\n","# طلب إدخال النص من المستخدم\n","text = input(\"Enter Text: \")\n","\n","# طلب إدخال اللغة المستهدفة من المستخدم\n","target_lang = input(\"Enter Target Language (e.g., fr, ar, es): \")\n","\n","# ترجمة النص باستخدام المكتبة\n","translated = translator.translate(text, dest=target_lang)\n","\n","# عرض النص المترجم\n","print(translated.text)"],"metadata":{"id":"Oj9BexrtWokR"},"execution_count":null,"outputs":[]}]}